{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR-10 Convolutional neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise - Load data\n",
    "\n",
    "> **Exercise**: Load the CIFAR-10 data. Normalize the images and split them into train, validation and test sets. Define a `get_batches(X, y, batch_size)` function to generate random X/y batches of size `batch_size` using a Python generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (9500, 32, 32, 3) (9500,)\n",
      "Valid: (500, 32, 32, 3) (500,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# loading the data\n",
    "with np.load(os.path.join('data', 'cifar10-10k.npz'), allow_pickle=False) as npz_file:\n",
    "    cifar10 = dict(npz_file.items())\n",
    "\n",
    "\n",
    "# changing the data type from uint8 to float32\n",
    "data = cifar10['data'].astype(np.float32)\n",
    "\n",
    "\n",
    "# standardize the images\n",
    "data = (data-128)/255\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split into train and validation sets\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    # Reshape images (32 by 32)\n",
    "    data.reshape(-1, 32, 32, 3), # 3 channels RGB\n",
    "    cifar10['labels'],\n",
    "    test_size=500, random_state=0\n",
    ")\n",
    "\n",
    "# Print shape\n",
    "print('Train:', X_train.shape, y_train.shape)\n",
    "# Returns: (19500, 28, 28, 1) (19500,)\n",
    "print('Valid:', X_valid.shape, y_valid.shape)\n",
    "# Returns: (500, 28, 28, 1) (500,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch generator\n",
    "def get_batches(X, y, batch_size):\n",
    "    # Shuffle X,y\n",
    "    shuffled_idx = np.arange(len(y)) # 1,2,...,n\n",
    "    np.random.shuffle(shuffled_idx)\n",
    "\n",
    "    # Enumerate indexes by steps of batch_size\n",
    "    # i: 0, b, 2b, 3b, 4b, .. where b is the batch size\n",
    "    for i in range(0, len(y), batch_size):\n",
    "        # Batch indexes\n",
    "        batch_idx = shuffled_idx[i:i+batch_size]\n",
    "        yield X[batch_idx], y[batch_idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise - Create and train a ConvNet\n",
    "\n",
    "> **Exercise:** Create a convolutional neural network and train it using your batch generator. Evaluate the accuracy on the validation set after each epoch. Test different architectures and parameters. Evaluate your best network on the test set. Save the trained kernel weights of the first convolutional layer in a variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 16, 16, 128)\n",
      "(?, 8, 8, 128)\n",
      "(?, 8, 8, 16)\n",
      "(?, 4, 4, 16)\n",
      "(?, 4, 4, 8)\n",
      "(?, 2, 2, 8)\n",
      "(?, 256)\n",
      "(?, 10)\n",
      "<tf.Variable 'hidden/kernel:0' shape=(5, 5, 3, 128) dtype=float32_ref>\n",
      "<tf.Variable 'hidden/bias:0' shape=(128,) dtype=float32_ref>\n",
      "<tf.Variable 'conv2/kernel:0' shape=(3, 3, 128, 16) dtype=float32_ref>\n",
      "<tf.Variable 'conv2/bias:0' shape=(16,) dtype=float32_ref>\n",
      "<tf.Variable 'conv3/kernel:0' shape=(3, 3, 16, 8) dtype=float32_ref>\n",
      "<tf.Variable 'conv3/bias:0' shape=(8,) dtype=float32_ref>\n",
      "<tf.Variable 'dense/kernel:0' shape=(256, 10) dtype=float32_ref>\n",
      "<tf.Variable 'dense/bias:0' shape=(10,) dtype=float32_ref>\n",
      "(5, 5, 3, 128)\n",
      "(3, 3, 128, 16)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Redefine graph\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    # Placeholders\n",
    "    X = tf.placeholder(dtype=tf.float32, shape=[None, 32, 32, 3])\n",
    "    y = tf.placeholder(dtype=tf.int32, shape=[None])\n",
    "\n",
    "    hidden = tf.layers.conv2d(\n",
    "        X, # Input data\n",
    "        filters=128, # 64 filters\n",
    "        kernel_size=(5, 5), # Kernel size: 5x5\n",
    "        strides=(2, 2), # Stride: 2\n",
    "        padding='SAME', # \"same\" padding\n",
    "        activation=tf.nn.relu, # ReLU\n",
    "        kernel_initializer=tf.truncated_normal_initializer(\n",
    "            stddev=0.01, seed=0), # Small standard deviation\n",
    "        name='hidden' # Add name\n",
    "    )    \n",
    "    \n",
    "    # Convolutional layer\n",
    "    #conv = tf.layers.conv2d(\n",
    "    #    X, # Input data\n",
    "    #    filters=64, # 64 filters\n",
    "    #    kernel_size=(5, 5), # Kernel size: 5x5\n",
    "    #    strides=(2, 2), # Stride: 2\n",
    "    #    padding='SAME', # \"same\" padding\n",
    "    #    activation=tf.nn.relu, # ReLU\n",
    "    #    kernel_initializer=tf.truncated_normal_initializer(\n",
    "    #        stddev=0.01, seed=0), # Small standard deviation\n",
    "    #    name='conv' # Add name\n",
    "    #)\n",
    "\n",
    "print(hidden.shape) # Prints: (?, 14, 14, 16)\n",
    "\n",
    "with graph.as_default():\n",
    "    # Max pooling layer\n",
    "    pool = tf.layers.max_pooling2d(\n",
    "        hidden,\n",
    "#        conv, # Convolution output\n",
    "        pool_size=(2, 2), # Pool size: 2\n",
    "        strides=(2, 2), # Stride: 2\n",
    "        padding='SAME' # \"same\" padding\n",
    "    )\n",
    "\n",
    "print(pool.shape) # Prints: (?, 7, 7, 16)\n",
    "\n",
    "with graph.as_default():\n",
    "    # Convolutional layer\n",
    "    conv2 = tf.layers.conv2d(\n",
    "        pool, # Max pooling output\n",
    "        filters=16, # 16 filters\n",
    "        kernel_size=(3, 3), # Kernel size: 3x3\n",
    "        strides=(1, 1), # Stride: 1\n",
    "        padding='SAME', # \"same\" padding\n",
    "        activation=tf.nn.relu, # ReLU\n",
    "        kernel_initializer=tf.truncated_normal_initializer(\n",
    "            stddev=0.01, seed=0), # Small standard deviation\n",
    "        name='conv2' # Add name\n",
    "    )\n",
    "\n",
    "    # Max pooling layer (2x2, stride: 2)\n",
    "    pool2 = tf.layers.max_pooling2d(\n",
    "        conv2, pool_size=(2, 2), strides=(2, 2), padding='SAME')\n",
    "\n",
    "print(conv2.shape)\n",
    "print(pool2.shape)\n",
    "\n",
    "\n",
    "# adding a forth layer\n",
    "with graph.as_default():\n",
    "    # Convolutional layer\n",
    "    conv3 = tf.layers.conv2d(\n",
    "        pool2, # Max pooling output\n",
    "        filters=8, # 16 filters\n",
    "        kernel_size=(3, 3), # Kernel size: 3x3\n",
    "        strides=(1, 1), # Stride: 1\n",
    "        padding='SAME', # \"same\" padding\n",
    "        activation=tf.nn.relu, # ReLU\n",
    "        kernel_initializer=tf.truncated_normal_initializer(\n",
    "            stddev=0.01, seed=0), # Small standard deviation\n",
    "        name='conv3' # Add name\n",
    "    )\n",
    "\n",
    "    # Max pooling layer (2x2, stride: 2)\n",
    "    pool3 = tf.layers.max_pooling2d(\n",
    "        conv3, pool_size=(2, 2), strides=(2, 2), padding='SAME')\n",
    "\n",
    "print(conv3.shape)\n",
    "print(pool3.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with graph.as_default():\n",
    "    # Flatten output\n",
    "    flat_output = tf.contrib.layers.flatten(pool2)\n",
    "\n",
    "print(flat_output.shape)\n",
    "\n",
    "\n",
    "# adding the dropout\n",
    "with graph.as_default():\n",
    "    # placeholder\n",
    "    training = tf.placeholder(dtype=tf.bool)\n",
    "    \n",
    "    # apply dropout\n",
    "    hidden = tf.layers.dropout(\n",
    "        hidden, rate=0.5, seed=0, training=training)\n",
    "\n",
    "\n",
    "with graph.as_default():\n",
    "    # Output layer\n",
    "    logits = tf.layers.dense(\n",
    "        flat_output, 10, # Output units: 10\n",
    "        activation=None, # No activation function\n",
    "        kernel_initializer=tf.variance_scaling_initializer(scale=1, seed=0),\n",
    "        bias_initializer=tf.zeros_initializer(),\n",
    "        name='dense'\n",
    "    )\n",
    "\n",
    "print(logits.shape) # Prints: (?, 10)\n",
    "\n",
    "\n",
    "with graph.as_default():\n",
    "    # Mean cross-entropy\n",
    "    mean_ce = tf.reduce_mean(\n",
    "        tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            labels=y, logits=logits))\n",
    "\n",
    "    # Adam optimizer\n",
    "    lr = tf.placeholder(dtype=tf.float32)\n",
    "    gd = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "\n",
    "    # Minimize cross-entropy\n",
    "    train_op = gd.minimize(mean_ce)\n",
    "\n",
    "    # Compute predictions and accuracy\n",
    "    predictions = tf.argmax(logits, axis=1, output_type=tf.int32)\n",
    "    is_correct = tf.equal(y, predictions)\n",
    "    accuracy = tf.reduce_mean(tf.cast(is_correct, dtype=tf.float32))\n",
    "\n",
    "\n",
    "#Looking at the variables in the chart\n",
    "with graph.as_default():\n",
    "    # Get variables in the graph\n",
    "    for v in tf.trainable_variables():\n",
    "        print(v)\n",
    "\n",
    "with graph.as_default():\n",
    "    # Kernel weights of the 1st conv. layer\n",
    "    with tf.variable_scope('hidden', reuse=True):\n",
    "        conv_kernels = tf.get_variable('kernel')\n",
    "print(conv_kernels.shape)\n",
    "\n",
    "with graph.as_default():\n",
    "    # Kernel weights of the 1st conv. layer\n",
    "    with tf.variable_scope('conv2', reuse=True):\n",
    "        conv2_kernels = tf.get_variable('kernel')\n",
    "\n",
    "print(conv2_kernels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - valid: 0.326 train: 0.257 (mean)\n",
      "Epoch 2 - valid: 0.418 train: 0.385 (mean)\n",
      "Epoch 3 - valid: 0.436 train: 0.436 (mean)\n",
      "Epoch 4 - valid: 0.468 train: 0.477 (mean)\n",
      "Epoch 5 - valid: 0.504 train: 0.495 (mean)\n"
     ]
    }
   ],
   "source": [
    "#Train the network\n",
    "\n",
    "# Validation accuracy\n",
    "valid_acc_values = []\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    # Initialize variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Set seed\n",
    "    np.random.seed(0)\n",
    "\n",
    "    # Train several epochs\n",
    "    for epoch in range(10):\n",
    "        # Accuracy values (train) after each batch\n",
    "        batch_acc = []\n",
    "\n",
    "        for X_batch, y_batch in get_batches(X_train, y_train, 128):\n",
    "            # Run training and evaluate accuracy\n",
    "            _, acc_value = sess.run([train_op, accuracy], feed_dict={\n",
    "                X: X_batch,\n",
    "                y: y_batch,\n",
    "                lr: 0.001, # Learning rate\n",
    "                training: True\n",
    "            })\n",
    "\n",
    "            # Save accuracy (current batch)\n",
    "            batch_acc.append(acc_value)\n",
    "\n",
    "        # Evaluate validation accuracy\n",
    "        valid_acc = sess.run(accuracy, feed_dict={\n",
    "            X: X_valid,\n",
    "            y: y_valid,\n",
    "            training: False\n",
    "        })\n",
    "        valid_acc_values.append(valid_acc)\n",
    "\n",
    "        # Print progress\n",
    "        print('Epoch {} - valid: {:.3f} train: {:.3f} (mean)'.format(\n",
    "            epoch+1, valid_acc, np.mean(batch_acc)\n",
    "        ))\n",
    "\n",
    "    # Get 1st conv. layer kernels\n",
    "    # get the convolutional layers kernels\n",
    "    ker = conv_kernels.eval()\n",
    "    ker2 = conv2_kernels.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(valid_acc_values)\n",
    "plt.title('Validation accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise - Visualize kernels\n",
    "\n",
    "> **Exercise**: Plot the kernels from the first convolutional layer with the `imshow()` function.\n",
    "\n",
    "**Hint**: Remember that the `imshow()` function expects values between 0 and 1 for 3-dimensional arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ker_norm = (ker / 255)\n",
    "#plt.imshow(ker_norm[1,1,:])\n",
    "#plt.imshow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ker.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(ker_norm[1,1,:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(ker_norm[:,:,0,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ker_norm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-1950c5288946>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# getting the kernel i\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mkernel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mker_norm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# plot kernel with imshow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ker_norm' is not defined"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#ker_norm = (ker / 255)\n",
    "\n",
    "# create figure with subplots\n",
    "\n",
    "fig, axes = plt.subplots(nrows=8, ncols=8, figsize=(8,8))\n",
    "for i, axis in enumerate(axes.flatten()):\n",
    "    # getting the kernel i\n",
    "    kernel = ker_norm[:,:,0,i]\n",
    "    \n",
    "    # plot kernel with imshow\n",
    "    plt.imshow(kernel)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
